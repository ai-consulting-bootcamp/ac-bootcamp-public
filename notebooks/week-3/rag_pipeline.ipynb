{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Pipeline Optimization\n",
        "\n",
        "This notebook demonstrates techniques to improve RAG pipeline performance: query improvement, reranking, and metadata filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!pip install langchain langchain-openai langchain-pinecone pinecone-client python-dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API keys are loaded\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables. Please create a .env file with your API key.\")\n",
        "if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "    raise ValueError(\"PINECONE_API_KEY not found in environment variables. Please create a .env file with your API key.\")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Query Improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query rewriting example\n",
        "def rewrite_query(original_query):\n",
        "    prompt = f\"Rewrite this query to be more specific and better suited for document retrieval: {original_query}\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "original = \"Tell me about AI\"\n",
        "rewritten = rewrite_query(original)\n",
        "print(f\"Original: {original}\")\n",
        "print(f\"Rewritten: {rewritten}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sub-query decomposition\n",
        "def decompose_query(complex_query):\n",
        "    prompt = f\"Break this complex query into 2-3 simpler sub-queries: {complex_query}\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "complex = \"How does machine learning compare to deep learning and what are their applications?\"\n",
        "sub_queries = decompose_query(complex)\n",
        "print(f\"Complex query: {complex}\")\n",
        "print(f\"Sub-queries: {sub_queries}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Test when query improvement helps vs when it's unnecessary complexity. Simple queries often work fine - only add query rewriting when you see consistent retrieval failures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Parent Document Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parent document retrieval pattern\n",
        "# 1. Store small chunks with parent IDs\n",
        "# 2. Retrieve small chunks (better semantic match)\n",
        "# 3. Fetch parent documents for full context\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Create parent documents\n",
        "parent_docs = [\n",
        "    Document(page_content=\"Machine learning is a subset of AI. Deep learning uses neural networks. NLP processes human language.\")\n",
        "]\n",
        "\n",
        "# Split into small chunks with parent metadata\n",
        "small_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
        "small_chunks = small_splitter.split_documents(parent_docs)\n",
        "\n",
        "# Add parent ID to each chunk\n",
        "for i, chunk in enumerate(small_chunks):\n",
        "    chunk.metadata[\"parent_id\"] = 0  # Reference to parent document\n",
        "\n",
        "print(f\"Created {len(small_chunks)} small chunks from 1 parent document\")\n",
        "print(f\"First chunk: {small_chunks[0].page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Recognize when parent document retrieval is needed vs when simple chunks work. Use this pattern when you need both precise semantic matching (small chunks) and full context (parent documents)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Relevance Scoring & Reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLM-based reranking\n",
        "def rerank_documents(query, documents, top_k=2):\n",
        "    # Score each document\n",
        "    scores = []\n",
        "    for doc in documents:\n",
        "        prompt = f\"Rate the relevance of this document to the query '{query}' on a scale of 0-1: {doc.page_content}\"\n",
        "        response = llm.invoke(prompt)\n",
        "        # Extract score (simplified - in practice, use structured output)\n",
        "        try:\n",
        "            score = float(response.content.split()[0])\n",
        "            scores.append((score, doc))\n",
        "        except:\n",
        "            scores.append((0.5, doc))\n",
        "    \n",
        "    # Sort by score and return top_k\n",
        "    scores.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [doc for score, doc in scores[:top_k]]\n",
        "\n",
        "# Example usage\n",
        "sample_docs = [\n",
        "    Document(page_content=\"Machine learning is a subset of artificial intelligence.\"),\n",
        "    Document(page_content=\"The weather today is sunny and warm.\"),\n",
        "    Document(page_content=\"Deep learning uses neural networks for pattern recognition.\")\n",
        "]\n",
        "\n",
        "query = \"What is machine learning?\"\n",
        "reranked = rerank_documents(query, sample_docs)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Top result: {reranked[0].page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Measure improvement vs cost - when reranking is justified. Reranking adds latency and cost. Use it when retrieval quality is critical, not for every query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Metadata Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add metadata to documents\n",
        "from datetime import datetime\n",
        "\n",
        "documents_with_metadata = [\n",
        "    Document(\n",
        "        page_content=\"New regulation on data privacy\",\n",
        "        metadata={\"date\": \"2024-01-15\", \"category\": \"regulations\", \"source\": \"legal\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Old regulation from 2020\",\n",
        "        metadata={\"date\": \"2020-03-10\", \"category\": \"regulations\", \"source\": \"legal\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Technical documentation\",\n",
        "        metadata={\"date\": \"2024-02-20\", \"category\": \"technical\", \"source\": \"docs\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "# Filter by metadata before retrieval\n",
        "def filter_by_metadata(docs, date_cutoff=None, category=None):\n",
        "    filtered = docs\n",
        "    if date_cutoff:\n",
        "        filtered = [d for d in filtered if d.metadata.get(\"date\", \"\") >= date_cutoff]\n",
        "    if category:\n",
        "        filtered = [d for d in filtered if d.metadata.get(\"category\") == category]\n",
        "    return filtered\n",
        "\n",
        "# Example: Get recent regulations only\n",
        "recent_regs = filter_by_metadata(documents_with_metadata, date_cutoff=\"2024-01-01\", category=\"regulations\")\n",
        "print(f\"Found {len(recent_regs)} recent regulations\")\n",
        "for doc in recent_regs:\n",
        "    print(f\"- {doc.page_content} ({doc.metadata['date']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: When metadata setup is worth the upfront cost vs when to skip it. Metadata filtering requires upfront work to tag documents. Use it when you have clear filtering needs (dates, categories, sources), not as a default."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "consulting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
