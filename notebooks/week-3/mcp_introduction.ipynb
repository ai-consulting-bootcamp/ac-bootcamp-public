{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Introduction with LangChain\n",
    "\n",
    "This notebook demonstrates connecting to MCP servers and integrating them with LangChain agents using the `langchain-mcp-adapters` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install langchain langchain-openai langchain-mcp-adapters langgraph mcp python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables. Please create a .env file with your API key.\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Integration with MCP\n",
    "\n",
    "### Using langchain-mcp-adapters\n",
    "\n",
    "LangChain provides the `langchain-mcp-adapters` package, which offers a seamless way to connect LangChain applications to MCP servers. This package includes `MultiServerMCPClient`, which allows you to connect to multiple MCP servers and load their tools, resources, and prompts into LangChain agents.\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Standardized Protocol**: Work with one protocol instead of juggling multiple APIs\n",
    "- **Multiple Servers**: Connect to multiple MCP servers simultaneously\n",
    "- **LangChain Native**: Tools, resources, and prompts are automatically LangChain-compatible\n",
    "- **Stateless by Default**: Each tool call creates a fresh session, no context manager needed\n",
    "- **Tool Interceptors**: Middleware pattern for customizing tool behavior\n",
    "\n",
    "### The LangChain Documentation MCP Server\n",
    "\n",
    "We'll use the LangChain documentation MCP server as our example:\n",
    "- **MCP Server URL**: `https://docs.langchain.com/mcp`\n",
    "- **Documentation Index**: `https://docs.langchain.com/llms.txt`\n",
    "- **Transport**: HTTP (not stdio)\n",
    "- **Provides**: Access to LangChain, LangGraph, and LangSmith documentation\n",
    "\n",
    "This MCP server allows you to query the latest LangChain documentation programmatically, making it perfect for building agents that can answer questions about LangChain features and APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the LangChain Documentation MCP Server\n",
    "\n",
    "Now let's connect to the LangChain documentation MCP server using HTTP transport.\n",
    "\n",
    "`MultiServerMCPClient` accepts a dictionary where keys are server names and values are configuration dicts specifying `transport` and connection details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "import asyncio\n",
    "\n",
    "# Configure the LangChain documentation MCP server\n",
    "# MultiServerMCPClient takes a dict of server configs\n",
    "# Each config specifies transport type and connection details\n",
    "mcp_client = MultiServerMCPClient({\n",
    "    \"langchain-docs\": {\n",
    "        \"transport\": \"http\",\n",
    "        \"url\": \"https://docs.langchain.com/mcp\"\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"MCP client configured for LangChain documentation server\")\n",
    "print(f\"Server: langchain-docs → https://docs.langchain.com/mcp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MCP Tools into LangChain\n",
    "\n",
    "The `get_tools()` method retrieves all tools from connected MCP servers and converts them to LangChain-compatible tools.\n",
    "\n",
    "**Note:** `MultiServerMCPClient` is stateless by default — each call to `get_tools()` internally creates and tears down fresh sessions automatically. No `async with` context manager is needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tools from the MCP server\n",
    "# The client is stateless: get_tools() creates ephemeral sessions under the hood\n",
    "mcp_tools = await mcp_client.get_tools()\n",
    "\n",
    "print(f\"Loaded {len(mcp_tools)} tools from MCP server(s)\")\n",
    "print(\"\\nAvailable tools:\")\n",
    "for tool in mcp_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MCP Tools with LangChain Agents\n",
    "\n",
    "Now let's create a LangChain agent that can use these MCP tools to query the LangChain documentation.\n",
    "\n",
    "We use `create_react_agent` from `langgraph.prebuilt` with the `prompt` parameter to set the system message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Get tools from MCP server\n",
    "tools = await mcp_client.get_tools()\n",
    "\n",
    "# Create agent with the model and MCP tools\n",
    "# Use the 'prompt' parameter (not 'state_modifier', which is deprecated)\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful assistant that can answer questions about LangChain, LangGraph, and LangSmith by searching the official documentation. Always provide accurate, up-to-date information based on the documentation.\"\n",
    ")\n",
    "\n",
    "print(f\"Agent created with {len(tools)} MCP tools\")\n",
    "print(f\"Tools: {[tool.name for tool in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying LangChain Documentation\n",
    "\n",
    "Let's test the agent by asking it questions about LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the agent about LangChain\n",
    "question = \"How do I create a LangChain agent with tools?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "result = await agent.ainvoke({\n",
    "    \"messages\": [HumanMessage(content=question)]\n",
    "})\n",
    "\n",
    "print(f\"Answer: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing MCP Resources\n",
    "\n",
    "MCP resources provide read-only access to data. The `get_resources()` method returns LangChain `Blob` objects.\n",
    "\n",
    "You can optionally filter by server name or specific URIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access MCP resources\n",
    "# get_resources() is also stateless — no context manager needed\n",
    "mcp_resources = await mcp_client.get_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_tools = await mcp_client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middleware and Tool Interceptors\n",
    "\n",
    "One of the powerful features of `langchain-mcp-adapters` is the ability to use tool interceptors (middleware) to customize tool behavior. This allows you to:\n",
    "\n",
    "- **Log tool calls**: Track when and how tools are used\n",
    "- **Modify requests/responses**: Transform tool inputs or outputs\n",
    "- **Add validation**: Ensure tool inputs meet certain criteria\n",
    "- **Implement retry logic, caching, or rate limiting**\n",
    "\n",
    "Interceptors follow a handler callback pattern (similar to middleware in web frameworks). Any async callable matching the `(request, handler) -> result` signature works. Interceptors compose in an \"onion\" pattern where the first interceptor is the outermost layer.\n",
    "\n",
    "Let's create a simple tool interceptor that logs tool usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Tool interceptors are async callables with signature:\n",
    "#   async def interceptor(request, handler) -> result\n",
    "# - request: MCPToolCallRequest with .name and .arguments\n",
    "# - handler: callable to invoke the next interceptor or the actual tool\n",
    "# - return: MCPToolCallResult\n",
    "\n",
    "async def logging_interceptor(request, handler):\n",
    "    \"\"\"Interceptor that logs tool calls and responses.\"\"\"\n",
    "    print(f\"[LOG] Tool '{request.name}' called with input: {request.args}\")\n",
    "    result = await handler(request)\n",
    "    print(f\"[LOG] Tool '{request.name}' returned result: {str(result.content)[:100]}...\")\n",
    "    return result\n",
    "\n",
    "# Create MCP client with tool interceptor\n",
    "# Pass interceptors as a list via the tool_interceptors parameter\n",
    "mcp_client_with_logging = MultiServerMCPClient(\n",
    "    {\n",
    "        \"langchain-docs\": {\n",
    "            \"transport\": \"http\",\n",
    "            \"url\": \"https://docs.langchain.com/mcp\"\n",
    "        }\n",
    "    },\n",
    "    tool_interceptors=[logging_interceptor]\n",
    ")\n",
    "\n",
    "print(\"MCP client configured with logging interceptor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Example: Agent with Logging\n",
    "\n",
    "Let's create a complete example that uses the logging interceptor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tools (interceptor will log tool calls)\n",
    "tools = await mcp_client_with_logging.get_tools()\n",
    "\n",
    "# Create agent\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful assistant that answers questions about LangChain documentation.\"\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is LangChain and what are its main features?\"\n",
    "print(f\"\\nQuestion: {question}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = await agent.ainvoke({\n",
    "    \"messages\": [HumanMessage(content=question)]\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFinal Answer:\\n{result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **langchain-mcp-adapters** provides a seamless way to integrate MCP servers with LangChain\n",
    "2. **MultiServerMCPClient** accepts a dict of server configurations — no separate parameter classes needed\n",
    "3. **Stateless by default** — each `get_tools()` or `get_resources()` call handles sessions internally, no `async with` needed\n",
    "4. **HTTP Transport** is used for web-based MCP servers like the LangChain documentation server\n",
    "5. **Tool Interceptors** are async callables with `(request, handler) -> result` signature, passed via `tool_interceptors=`\n",
    "6. **`create_react_agent`** uses the `prompt` parameter (not `state_modifier`, which is deprecated)\n",
    "\n",
    "### Documentation References\n",
    "\n",
    "- **LangChain Documentation MCP**: `https://docs.langchain.com/mcp`\n",
    "- **Documentation Index**: `https://docs.langchain.com/llms.txt`\n",
    "- **langchain-mcp-adapters**: [LangChain MCP Documentation](https://docs.langchain.com/oss/python/langchain/mcp)\n",
    "- **API Reference**: [langchain-mcp-adapters Reference](https://reference.langchain.com/python/langchain_mcp_adapters/)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore other MCP servers from the [official MCP servers repository](https://github.com/modelcontextprotocol/servers)\n",
    "- Build custom tool interceptors for your specific use cases\n",
    "- Combine multiple MCP servers for comprehensive agent capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "consulting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
