{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain 1.0 Integration\n",
        "\n",
        "## The Framework to Rule All of the APIs\n",
        "\n",
        "Welcome to this comprehensive guide on LangChain 1.0! This notebook will walk you through all the core concepts of LangChain, the library that reconciles different packages in the LLM space.\n",
        "\n",
        "### What You'll Learn\n",
        "- **Models**: Abstractions for LLMs with invoke, stream, and batch capabilities\n",
        "- **Tools**: Connecting models to the real world\n",
        "- **Agents**: LLM + Tools + Autonomy\n",
        "- **Memory**: Managing conversation context\n",
        "- **Messages**: The unit of context\n",
        "- **Advanced Features**: Prompt caching, rate limiting, and more\n",
        "\n",
        "### The Trade-off\n",
        "Instead of learning every single API (OpenAI, Anthropic, Gemini, Cohere, Pinecone, Chroma, etc.), you learn to reason in the LangChain Framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Installation\n",
        "\n",
        "First, let's install the necessary packages for LangChain 1.0:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-openai langchain-anthropic langchain-core langgraph python-dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API key is loaded\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables. Please create a .env file with your API key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Models\n",
        "\n",
        "### What is a Model?\n",
        "A **Model** is an abstraction of LLMs in LangChain. Models have:\n",
        "- **Tool calling** → calling external tools\n",
        "- **Structured Output** → model response follows a defined format\n",
        "- **Multimodality** → process more than text (images, audio, video)\n",
        "- **Reasoning** → performs multi-step reasoning\n",
        "\n",
        "### What Models Can Do:\n",
        "- **Invoke** → generate response and output a message\n",
        "- **Stream** → generate response and output it in real time\n",
        "- **Batch** → send multiple requests (improves processing time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: Using `init_chat_model` (Unified Interface)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize a model using the unified interface\n",
        "model = init_chat_model(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    model_provider=\"openai\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(f\"Model initialized: {model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2: Using Model Classes Directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize using the class directly\n",
        "chat_model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(f\"ChatOpenAI model: {chat_model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Operation 1: Invoke\n",
        "\n",
        "**Invoke** generates a complete response and outputs a message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple invoke example\n",
        "response = model.invoke(\"Explain what LangChain is in one sentence.\")\n",
        "print(f\"Response: {response.content}\")\n",
        "print(f\"\\nFull response object: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Operation 2: Stream\n",
        "\n",
        "**Stream** generates a response and outputs it in real-time, which is great for user experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stream example\n",
        "print(\"Streaming response:\")\n",
        "for chunk in model.stream(\"Tell me a short story about AI in 3 sentences.\"):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Operation 3: Batch\n",
        "\n",
        "**Batch** sends multiple requests at once to improve processing time.\n",
        "\n",
        "#### Important Notes on Batching:\n",
        "- LangChain parallelizes model calls on the **CLIENT-SIDE**\n",
        "- Some APIs support batching by the **INFERENCE provider**\n",
        "- Results can arrive out of order with `batch_as_completed` and need to be matched by IDs\n",
        "- Use `max_concurrency` to control concurrent calls and avoid overwhelming the API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch example\n",
        "questions = [\n",
        "    \"What is machine learning?\",\n",
        "    \"What is deep learning?\",\n",
        "    \"What is natural language processing?\"\n",
        "]\n",
        "\n",
        "# Batch process all questions\n",
        "responses = model.batch(questions)\n",
        "\n",
        "for i, response in enumerate(responses):\n",
        "    print(f\"Q{i+1}: {questions[i]}\")\n",
        "    print(f\"A{i+1}: {response.content}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch with max_concurrency to control rate\n",
        "responses = model.batch(\n",
        "    questions,\n",
        "    config={\"max_concurrency\": 2}  # Only 2 concurrent requests\n",
        ")\n",
        "\n",
        "print(f\"Processed {len(responses)} responses with controlled concurrency\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Messages\n",
        "\n",
        "### The Unit of Context\n",
        "\n",
        "Messages hold the input and output of the model. Each message has:\n",
        "- **Role**: Who is speaking\n",
        "- **Content**: What is being said\n",
        "- **Metadata**: Additional information\n",
        "\n",
        "### Message Types:\n",
        "1. **System Message** → context and model instructions\n",
        "2. **Human Message** → user input\n",
        "3. **AI Message** → message by the LLM\n",
        "4. **Tool Message** → output of tool calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "# Create different message types\n",
        "system_msg = SystemMessage(content=\"You are a helpful AI assistant specializing in Python programming.\")\n",
        "human_msg = HumanMessage(content=\"How do I create a list in Python?\")\n",
        "ai_msg = AIMessage(content=\"You can create a list in Python using square brackets: my_list = [1, 2, 3]\")\n",
        "\n",
        "print(\"System Message:\")\n",
        "print(f\"  Role: {system_msg.type}\")\n",
        "print(f\"  Content: {system_msg.content}\")\n",
        "print(\"\\nHuman Message:\")\n",
        "print(f\"  Role: {human_msg.type}\")\n",
        "print(f\"  Content: {human_msg.content}\")\n",
        "print(\"\\nAI Message:\")\n",
        "print(f\"  Role: {ai_msg.type}\")\n",
        "print(f\"  Content: {ai_msg.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Messages with Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Invoke model with messages\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that explains concepts concisely.\"),\n",
        "    HumanMessage(content=\"What is LangChain?\")\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)\n",
        "print(f\"Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conversation with Message History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-turn conversation\n",
        "conversation = [\n",
        "    SystemMessage(content=\"You are a helpful AI tutor.\"),\n",
        "    HumanMessage(content=\"What is a variable in programming?\"),\n",
        "]\n",
        "\n",
        "# First response\n",
        "response1 = model.invoke(conversation)\n",
        "conversation.append(AIMessage(content=response1.content))\n",
        "\n",
        "# Follow-up question\n",
        "conversation.append(HumanMessage(content=\"Can you give me an example in Python?\"))\n",
        "response2 = model.invoke(conversation)\n",
        "\n",
        "print(\"Conversation:\")\n",
        "for msg in conversation:\n",
        "    print(f\"\\n{msg.type.upper()}: {msg.content}\")\n",
        "print(f\"\\nAI: {response2.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Tools\n",
        "\n",
        "### Connecting Models to the Real World\n",
        "\n",
        "**Tools** connect the models to the real world. They require:\n",
        "- **Tool Schema** (name, description, argument definitions)\n",
        "- **Function or coroutine** to execute\n",
        "\n",
        "#### Key Terms:\n",
        "- **Tool calling**, **function calling** → all mean the same thing!\n",
        "- **bind_tools** → allows the model to CHOOSE to use the tool\n",
        "- **Coroutine** → a special function that can pause, let other code run, and continue where it left off (great for APIs and LLMs)\n",
        "\n",
        "### Tool Execution Flow:\n",
        "1. Model calls the tool\n",
        "2. You execute the tool\n",
        "3. You give the result back to the model\n",
        "\n",
        "**Note:** If you want this to happen automatically, you need an **AGENT**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Simple Tool with the @tool Decorator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "\n",
        "# Simple tool using @tool decorator\n",
        "@tool\n",
        "def get_current_time(timezone: str = \"UTC\") -> str:\n",
        "    \"\"\"Get the current time in a specific timezone.\n",
        "    \n",
        "    Args:\n",
        "        timezone: The timezone to get the time for (default: UTC)\n",
        "    \n",
        "    Returns:\n",
        "        The current time as a string\n",
        "    \"\"\"\n",
        "    current_time = datetime.now()\n",
        "    return f\"The current time in {timezone} is {current_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "@tool\n",
        "def calculate_sum(a: float, b: float) -> float:\n",
        "    \"\"\"Calculate the sum of two numbers.\n",
        "    \n",
        "    Args:\n",
        "        a: First number\n",
        "        b: Second number\n",
        "    \n",
        "    Returns:\n",
        "        The sum of a and b\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "@tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get the weather for a specific city.\n",
        "    \n",
        "    Args:\n",
        "        city: The name of the city\n",
        "    \n",
        "    Returns:\n",
        "        A description of the weather (simulated)\n",
        "    \"\"\"\n",
        "    # Simulated weather data\n",
        "    return f\"The weather in {city} is sunny with a temperature of 72°F.\"\n",
        "\n",
        "print(\"Tools created:\")\n",
        "print(f\"1. {get_current_time.name}: {get_current_time.description}\")\n",
        "print(f\"2. {calculate_sum.name}: {calculate_sum.description}\")\n",
        "print(f\"3. {get_weather.name}: {get_weather.description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Tools with Pydantic Models (Complex Inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class SearchInput(BaseModel):\n",
        "    \"\"\"Input schema for search tool.\"\"\"\n",
        "    query: str = Field(description=\"The search query\")\n",
        "    max_results: int = Field(default=5, description=\"Maximum number of results to return\")\n",
        "    category: str = Field(default=\"general\", description=\"Search category (general, news, images)\")\n",
        "\n",
        "@tool(args_schema=SearchInput)\n",
        "def search_web(query: str, max_results: int = 5, category: str = \"general\") -> str:\n",
        "    \"\"\"Search the web for information.\n",
        "    \n",
        "    Returns:\n",
        "        Simulated search results\n",
        "    \"\"\"\n",
        "    return f\"Found {max_results} results for '{query}' in {category} category: [Result 1], [Result 2], ...\"\n",
        "\n",
        "print(f\"Complex tool created: {search_web.name}\")\n",
        "print(f\"Args schema: {search_web.args_schema.schema()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binding Tools to Models\n",
        "\n",
        "Use **bind_tools** to allow the model to choose when to use tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bind tools to the model\n",
        "tools = [get_current_time, calculate_sum, get_weather]\n",
        "model_with_tools = model.bind_tools(tools)\n",
        "\n",
        "print(f\"Model now has access to {len(tools)} tools\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tool Calling Example (Manual Execution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask the model a question that requires a tool\n",
        "response = model_with_tools.invoke(\"What's 25 + 37?\")\n",
        "\n",
        "print(f\"Response type: {type(response)}\")\n",
        "print(f\"Content: {response.content}\")\n",
        "\n",
        "# Check if the model wants to use a tool\n",
        "if response.tool_calls:\n",
        "    print(\"\\nTool calls requested:\")\n",
        "    for tool_call in response.tool_calls:\n",
        "        print(f\"  Tool: {tool_call['name']}\")\n",
        "        print(f\"  Arguments: {tool_call['args']}\")\n",
        "        print(f\"  ID: {tool_call['id']}\")\n",
        "        \n",
        "        # Execute the tool manually\n",
        "        if tool_call['name'] == 'calculate_sum':\n",
        "            result = calculate_sum.invoke(tool_call['args'])\n",
        "            print(f\"  Result: {result}\")\n",
        "else:\n",
        "    print(\"No tool calls made\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiple Tool Calls Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question requiring multiple tools\n",
        "response = model_with_tools.invoke(\n",
        "    \"What's the weather in Paris and what's the sum of 15 and 28?\"\n",
        ")\n",
        "\n",
        "print(\"Model response:\")\n",
        "if response.tool_calls:\n",
        "    print(f\"Model wants to use {len(response.tool_calls)} tool(s):\\n\")\n",
        "    for i, tool_call in enumerate(response.tool_calls, 1):\n",
        "        print(f\"Tool Call {i}:\")\n",
        "        print(f\"  Name: {tool_call['name']}\")\n",
        "        print(f\"  Args: {tool_call['args']}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Agents\n",
        "\n",
        "### LLM + Tools + Autonomy\n",
        "\n",
        "An **Agent** is a loop where the LLM runs its tools and tries to achieve user requests.\n",
        "\n",
        "#### Key Features:\n",
        "- The LLM stops when it achieves its goal OR when the iteration limit is reached\n",
        "- Created using `create_agent` (which uses LangGraph)\n",
        "- Can handle multiple tools, parallel tool calling, dynamic tools, and error handling\n",
        "\n",
        "#### Components:\n",
        "- **Model** (static or dynamic)\n",
        "- **Tools** (multiple tools, parallel calling, retry logic)\n",
        "- **System Prompt**\n",
        "- **Memory**\n",
        "- **Streaming**\n",
        "- **Middleware** (modify original behavior)\n",
        "\n",
        "**Important:** Tools can access runtime information!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "# Create an agent with tools\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=tools,\n",
        "    system_prompt=\"You are a helpful assistant that can check the time, calculate sums, and get weather information.\"\n",
        ")\n",
        "\n",
        "print(\"Agent created with ReAct pattern\")\n",
        "print(f\"Tools available: {[tool.name for tool in tools]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple agent execution\n",
        "result = agent.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"What's 42 + 58?\")]\n",
        "})\n",
        "\n",
        "print(\"Agent execution result:\")\n",
        "print(f\"Final answer: {result['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complex agent execution with multiple steps\n",
        "result = agent.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"What's the weather in London? Also, what's the current time?\")]\n",
        "})\n",
        "\n",
        "print(\"\\nAgent conversation:\")\n",
        "for i, message in enumerate(result['messages']):\n",
        "    if hasattr(message, 'content') and message.content:\n",
        "        print(f\"\\n[{message.type.upper()}]: {message.content}\")\n",
        "    if hasattr(message, 'tool_calls') and message.tool_calls:\n",
        "        print(f\"\\n[TOOL CALLS]:\")\n",
        "        for tc in message.tool_calls:\n",
        "            print(f\"  - {tc['name']}({tc['args']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streaming Agent Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stream agent execution\n",
        "print(\"Streaming agent response:\\n\")\n",
        "\n",
        "for chunk in agent.stream({\n",
        "    \"messages\": [HumanMessage(content=\"Calculate the sum of 100 and 250, then tell me about it.\")]\n",
        "}):\n",
        "    if 'agent' in chunk:\n",
        "        messages = chunk['agent'].get('messages', [])\n",
        "        for msg in messages:\n",
        "            if hasattr(msg, 'content') and msg.content:\n",
        "                print(f\"[{msg.type}]: {msg.content}\")\n",
        "    if 'tools' in chunk:\n",
        "        print(f\"[Tool execution]: {chunk['tools']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Memory\n",
        "\n",
        "### Short-Term Memory in Conversations\n",
        "\n",
        "**Memory** refers to the model's capacity to remember previous interactions within a single conversation.\n",
        "\n",
        "#### Issues with Memory:\n",
        "- LLMs perform poorly with too much information\n",
        "- More context = slower, more costly, and worse quality\n",
        "\n",
        "#### LangChain's Approach:\n",
        "- Saves messages in an **AgentState**\n",
        "- **Production** → store in a database\n",
        "- **MVPs/Testing** → saved in RAM memory\n",
        "- You can build custom memory by inheriting the class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Memory with Agents (Built-in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The agent automatically maintains memory across the conversation\n",
        "# Each invoke maintains state within that call\n",
        "\n",
        "conversation_state = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=\"My name is Alice and I live in Tokyo.\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "# First interaction\n",
        "result1 = agent.invoke(conversation_state)\n",
        "print(\"First interaction:\")\n",
        "print(result1['messages'][-1].content)\n",
        "\n",
        "# Add to conversation\n",
        "conversation_state['messages'] = result1['messages']\n",
        "conversation_state['messages'].append(HumanMessage(content=\"What city did I say I live in?\"))\n",
        "\n",
        "# Second interaction (should remember)\n",
        "result2 = agent.invoke(conversation_state)\n",
        "print(\"\\nSecond interaction:\")\n",
        "print(result2['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Memory Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple in-memory conversation history\n",
        "class ConversationMemory:\n",
        "    def __init__(self, max_messages: int = 10):\n",
        "        self.messages = []\n",
        "        self.max_messages = max_messages\n",
        "    \n",
        "    def add_message(self, message):\n",
        "        \"\"\"Add a message to memory.\"\"\"\n",
        "        self.messages.append(message)\n",
        "        # Keep only the last max_messages\n",
        "        if len(self.messages) > self.max_messages:\n",
        "            # Keep system message if it exists\n",
        "            system_msgs = [m for m in self.messages if isinstance(m, SystemMessage)]\n",
        "            other_msgs = [m for m in self.messages if not isinstance(m, SystemMessage)]\n",
        "            self.messages = system_msgs + other_msgs[-self.max_messages:]\n",
        "    \n",
        "    def get_messages(self):\n",
        "        \"\"\"Get all messages.\"\"\"\n",
        "        return self.messages\n",
        "    \n",
        "    def clear(self):\n",
        "        \"\"\"Clear all messages.\"\"\"\n",
        "        self.messages = []\n",
        "\n",
        "# Create memory\n",
        "memory = ConversationMemory(max_messages=6)\n",
        "\n",
        "# Add system message\n",
        "memory.add_message(SystemMessage(content=\"You are a helpful assistant.\"))\n",
        "\n",
        "# Conversation with memory\n",
        "memory.add_message(HumanMessage(content=\"I'm learning Python.\"))\n",
        "response1 = model.invoke(memory.get_messages())\n",
        "memory.add_message(AIMessage(content=response1.content))\n",
        "\n",
        "print(\"First exchange:\")\n",
        "print(f\"Human: I'm learning Python.\")\n",
        "print(f\"AI: {response1.content}\")\n",
        "\n",
        "# Follow-up\n",
        "memory.add_message(HumanMessage(content=\"What programming language am I learning?\"))\n",
        "response2 = model.invoke(memory.get_messages())\n",
        "memory.add_message(AIMessage(content=response2.content))\n",
        "\n",
        "print(\"\\nSecond exchange:\")\n",
        "print(f\"Human: What programming language am I learning?\")\n",
        "print(f\"AI: {response2.content}\")\n",
        "\n",
        "print(f\"\\nTotal messages in memory: {len(memory.get_messages())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Advanced Features\n",
        "\n",
        "### Prompt Caching\n",
        "\n",
        "**Prompt Caching** reduces latency and cost on repeating tokens.\n",
        "\n",
        "#### Two Types:\n",
        "1. **Implicit**: The provider does this automatically (OpenAI, Gemini)\n",
        "2. **Explicit**: You indicate the cache for more efficient savings\n",
        "   - ChatOpenAI → `prompt_cache_key`\n",
        "   - Anthropic → `AnthropicPromptCachingMiddleware`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with implicit caching (OpenAI)\n",
        "# The same system message will be cached automatically\n",
        "system_prompt = \"You are an expert Python programmer. You write clean, efficient, and well-documented code.\"\n",
        "\n",
        "# First call\n",
        "messages1 = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=\"Write a function to reverse a string.\")\n",
        "]\n",
        "response1 = model.invoke(messages1)\n",
        "\n",
        "# Second call with same system message (will benefit from caching)\n",
        "messages2 = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=\"Write a function to check if a number is prime.\")\n",
        "]\n",
        "response2 = model.invoke(messages2)\n",
        "\n",
        "print(\"Prompt caching example:\")\n",
        "print(\"Both calls use the same system message, which gets cached.\")\n",
        "print(f\"\\nResponse 1 excerpt: {response1.content[:100]}...\")\n",
        "print(f\"\\nResponse 2 excerpt: {response2.content[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rate Limiting\n",
        "\n",
        "Limits the number of invocations in a given time period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "class RateLimiter:\n",
        "    def __init__(self, max_calls: int, time_window: int):\n",
        "        \"\"\"Initialize rate limiter.\n",
        "        \n",
        "        Args:\n",
        "            max_calls: Maximum number of calls allowed\n",
        "            time_window: Time window in seconds\n",
        "        \"\"\"\n",
        "        self.max_calls = max_calls\n",
        "        self.time_window = time_window\n",
        "        self.calls = []\n",
        "    \n",
        "    def can_make_call(self) -> bool:\n",
        "        \"\"\"Check if a call can be made.\"\"\"\n",
        "        now = time.time()\n",
        "        # Remove old calls outside the time window\n",
        "        self.calls = [call_time for call_time in self.calls if now - call_time < self.time_window]\n",
        "        return len(self.calls) < self.max_calls\n",
        "    \n",
        "    def record_call(self):\n",
        "        \"\"\"Record a call.\"\"\"\n",
        "        self.calls.append(time.time())\n",
        "\n",
        "# Example: 3 calls per 10 seconds\n",
        "rate_limiter = RateLimiter(max_calls=3, time_window=10)\n",
        "\n",
        "print(\"Rate limiting example (3 calls per 10 seconds):\")\n",
        "for i in range(5):\n",
        "    if rate_limiter.can_make_call():\n",
        "        rate_limiter.record_call()\n",
        "        print(f\"Call {i+1}: Allowed at {datetime.now().strftime('%H:%M:%S')}\")\n",
        "    else:\n",
        "        print(f\"Call {i+1}: Rate limited at {datetime.now().strftime('%H:%M:%S')}\")\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token Usage Tracking\n",
        "\n",
        "Keeps track of tokens as part of the invocation response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Token usage is included in the response metadata\n",
        "response = model.invoke(\"Explain artificial intelligence in 2 sentences.\")\n",
        "\n",
        "print(\"Token usage information:\")\n",
        "if hasattr(response, 'usage_metadata'):\n",
        "    print(f\"Input tokens: {response.usage_metadata.get('input_tokens', 'N/A')}\")\n",
        "    print(f\"Output tokens: {response.usage_metadata.get('output_tokens', 'N/A')}\")\n",
        "    print(f\"Total tokens: {response.usage_metadata.get('total_tokens', 'N/A')}\")\n",
        "elif hasattr(response, 'response_metadata'):\n",
        "    print(f\"Response metadata: {response.response_metadata}\")\n",
        "else:\n",
        "    print(\"Token information not available in this response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration of Invocation\n",
        "\n",
        "Allows you to configure invocations via a dictionary, helpful for monitoring applications in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "# Configure the invocation\n",
        "config = RunnableConfig(\n",
        "    tags=[\"production\", \"user-query\"],\n",
        "    metadata={\n",
        "        \"user_id\": \"user_123\",\n",
        "        \"session_id\": \"session_456\",\n",
        "        \"environment\": \"production\"\n",
        "    },\n",
        "    max_concurrency=2\n",
        ")\n",
        "\n",
        "# Invoke with config\n",
        "response = model.invoke(\n",
        "    \"What is machine learning?\",\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(\"Invocation with configuration:\")\n",
        "print(f\"Response: {response.content[:100]}...\")\n",
        "print(f\"\\nConfiguration used:\")\n",
        "print(f\"  Tags: {config.get('tags')}\")\n",
        "print(f\"  Metadata: {config.get('metadata')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Structured Output\n",
        "\n",
        "One of the key features of models is the ability to generate **structured output** - responses that follow a defined format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Define output schema\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "    name: str = Field(description=\"The person's name\")\n",
        "    age: int = Field(description=\"The person's age\")\n",
        "    occupation: str = Field(description=\"The person's occupation\")\n",
        "    skills: List[str] = Field(description=\"List of the person's skills\")\n",
        "\n",
        "# Create a model with structured output\n",
        "structured_model = model.with_structured_output(Person)\n",
        "\n",
        "# Get structured output\n",
        "result = structured_model.invoke(\n",
        "    \"Tell me about Alice, a 28-year-old software engineer who knows Python, JavaScript, and machine learning.\"\n",
        ")\n",
        "\n",
        "print(\"Structured Output:\")\n",
        "print(f\"Name: {result.name}\")\n",
        "print(f\"Age: {result.age}\")\n",
        "print(f\"Occupation: {result.occupation}\")\n",
        "print(f\"Skills: {', '.join(result.skills)}\")\n",
        "print(f\"\\nType: {type(result)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# More complex structured output\n",
        "class Article(BaseModel):\n",
        "    \"\"\"A news article.\"\"\"\n",
        "    title: str = Field(description=\"Article title\")\n",
        "    summary: str = Field(description=\"Brief summary\")\n",
        "    key_points: List[str] = Field(description=\"Main points from the article\")\n",
        "    category: str = Field(description=\"Article category (tech, business, science, etc.)\")\n",
        "\n",
        "article_model = model.with_structured_output(Article)\n",
        "\n",
        "result = article_model.invoke(\n",
        "    \"Write an article about the recent advances in AI language models, focusing on their capabilities and impact.\"\n",
        ")\n",
        "\n",
        "print(\"Structured Article Output:\")\n",
        "print(f\"\\nTitle: {result.title}\")\n",
        "print(f\"\\nCategory: {result.category}\")\n",
        "print(f\"\\nSummary: {result.summary}\")\n",
        "print(f\"\\nKey Points:\")\n",
        "for i, point in enumerate(result.key_points, 1):\n",
        "    print(f\"  {i}. {point}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Coroutines and Async Operations\n",
        "\n",
        "A **coroutine** is a special type of function that can pause, let other code run, and continue where it left off.\n",
        "\n",
        "**Good for:** Increase responsiveness, handle tasks that involve waiting (like LLM and API calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "# Async tool example\n",
        "@tool\n",
        "async def async_search(query: str) -> str:\n",
        "    \"\"\"Asynchronously search for information.\n",
        "    \n",
        "    Args:\n",
        "        query: The search query\n",
        "    \n",
        "    Returns:\n",
        "        Simulated search results\n",
        "    \"\"\"\n",
        "    # Simulate async API call\n",
        "    await asyncio.sleep(1)\n",
        "    return f\"Search results for '{query}': [Result 1], [Result 2], [Result 3]\"\n",
        "\n",
        "# Async model invocation\n",
        "async def async_example():\n",
        "    # Multiple concurrent invocations\n",
        "    tasks = [\n",
        "        model.ainvoke(\"What is Python?\"),\n",
        "        model.ainvoke(\"What is JavaScript?\"),\n",
        "        model.ainvoke(\"What is Go?\")\n",
        "    ]\n",
        "    \n",
        "    responses = await asyncio.gather(*tasks)\n",
        "    \n",
        "    for i, response in enumerate(responses, 1):\n",
        "        print(f\"\\nResponse {i}: {response.content[:80]}...\")\n",
        "\n",
        "# Run async example\n",
        "print(\"Running async operations (3 concurrent model calls):\")\n",
        "await async_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9. Complete Agent Example with All Features\n",
        "\n",
        "Let's put it all together with a comprehensive example that demonstrates:\n",
        "- Multiple tools\n",
        "- Agent with memory\n",
        "- Structured output\n",
        "- Token tracking\n",
        "- Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define comprehensive tools\n",
        "@tool\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"Safely evaluate a mathematical expression.\n",
        "    \n",
        "    Args:\n",
        "        expression: A mathematical expression (e.g., '2 + 2', '10 * 5')\n",
        "    \n",
        "    Returns:\n",
        "        The result of the calculation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Only allow safe operations\n",
        "        allowed_chars = set('0123456789+-*/.() ')\n",
        "        if not all(c in allowed_chars for c in expression):\n",
        "            return \"Error: Expression contains invalid characters\"\n",
        "        result = eval(expression)\n",
        "        return f\"The result of {expression} is {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def get_user_info(user_id: str) -> str:\n",
        "    \"\"\"Get information about a user.\n",
        "    \n",
        "    Args:\n",
        "        user_id: The user's ID\n",
        "    \n",
        "    Returns:\n",
        "        User information (simulated)\n",
        "    \"\"\"\n",
        "    # Simulated user database\n",
        "    users = {\n",
        "        \"alice\": \"Alice (ID: alice) - Software Engineer, specializes in Python and AI\",\n",
        "        \"bob\": \"Bob (ID: bob) - Data Scientist, expert in machine learning\",\n",
        "        \"charlie\": \"Charlie (ID: charlie) - Product Manager, 5 years experience\"\n",
        "    }\n",
        "    return users.get(user_id.lower(), f\"User {user_id} not found\")\n",
        "\n",
        "@tool\n",
        "def create_task(title: str, priority: str = \"medium\") -> str:\n",
        "    \"\"\"Create a new task.\n",
        "    \n",
        "    Args:\n",
        "        title: The task title\n",
        "        priority: Task priority (low, medium, high)\n",
        "    \n",
        "    Returns:\n",
        "        Confirmation message\n",
        "    \"\"\"\n",
        "    task_id = hash(title) % 10000\n",
        "    return f\"Task created: '{title}' (ID: {task_id}, Priority: {priority})\"\n",
        "\n",
        "# Create comprehensive agent\n",
        "comprehensive_tools = [calculate, get_user_info, create_task, get_weather, get_current_time]\n",
        "\n",
        "comprehensive_agent = create_agent(\n",
        "    model=model,\n",
        "    tools=comprehensive_tools,\n",
        "    system_prompt=\"\"\"You are a helpful AI assistant with access to multiple tools.\n",
        "    You can perform calculations, get user information, create tasks, check weather, and get the current time.\n",
        "    Always be helpful and provide detailed responses.\"\"\"\n",
        ")\n",
        "\n",
        "print(\"Comprehensive agent created with tools:\")\n",
        "for tool in comprehensive_tools:\n",
        "    print(f\"  - {tool.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the comprehensive agent\n",
        "test_queries = [\n",
        "    \"Calculate 25 * 17 + 100\",\n",
        "    \"Get information about user alice and create a high priority task to review her code\",\n",
        "    \"What's the weather in San Francisco and what time is it?\"\n",
        "]\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Query {i}: {query}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    result = comprehensive_agent.invoke({\n",
        "        \"messages\": [HumanMessage(content=query)]\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nFinal Response: {result['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "consulting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
