{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Introduction with LangChain 1.0 (LCEL) and Pinecone\n",
        "\n",
        "This notebook demonstrates the complete RAG pipeline using **LangChain Expression Language (LCEL)**: document loading, chunking, embeddings, vector database setup, and retrieval-augmented generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-openai langchain-pinecone langchain-text-splitters pinecone-client pypdf python-dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API keys are loaded\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables. Please create a .env file with your API key.\")\n",
        "if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "    raise ValueError(\"PINECONE_API_KEY not found in environment variables. Please create a .env file with your API key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Document Loading and Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "# Load a PDF document (replace with your PDF path, do it together with the students on the second pass)\n",
        "# loader = PyPDFLoader(\"path/to/your/document.pdf\")\n",
        "# documents = loader.load()\n",
        "\n",
        "# For demonstration, create sample documents\n",
        "from langchain_core.documents import Document\n",
        "documents = [\n",
        "    Document(page_content=\"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\"),\n",
        "    Document(page_content=\"Deep learning uses neural networks with multiple layers to process complex patterns.\"),\n",
        "    Document(page_content=\"Natural language processing allows computers to understand and generate human language.\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chunking strategy 1: RecursiveCharacterTextSplitter (recommended)\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "print(f\"First chunk: {chunks[0].page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Chunking matters when documents are large or structured. For simple use cases with small documents, you might skip chunking entirely. Recognize when chunking adds value vs when it's unnecessary complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Create embeddings for a sample text\n",
        "sample_text = \"Machine learning enables systems to learn from data\"\n",
        "sample_embedding = embeddings.embed_query(sample_text)\n",
        "print(f\"Embedding dimension: {len(sample_embedding)}\")\n",
        "print(f\"First 5 values: {sample_embedding[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Embedding costs add up with large document collections. Consider cheaper embedding models for MVPs, and upgrade only when quality matters. Understand the cost implications before committing to a solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Pinecone Vector Store Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "\n",
        "# Create or connect to an index\n",
        "index_name = \"rag-intro-index\"\n",
        "\n",
        "# Check if index exists, create if not\n",
        "if index_name not in [index.name for index in pc.list_indexes()]:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,  # OpenAI text-embedding-3-small dimension\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "    print(f\"Created index: {index_name}\")\n",
        "else:\n",
        "    print(f\"Index {index_name} already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create vector store using LangChain\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "print(\"Documents added to Pinecone vector store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Pinecone is powerful but adds infrastructure complexity and cost. For small projects or MVPs, consider simpler alternatives like in-memory vector stores or Chroma. Use Pinecone when you need scale, performance, or managed infrastructure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Query and Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform a similarity search\n",
        "query = \"What is machine learning?\"\n",
        "results = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"\\nRetrieved {len(results)} documents:\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"\\n{i}. {doc.page_content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get similarity scores\n",
        "results_with_scores = vectorstore.similarity_search_with_score(query, k=2)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"\\nRetrieved documents with scores:\")\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"\\nScore: {score:.4f}\")\n",
        "    print(f\"Content: {doc.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Retrieval quality varies with chunking strategy and embedding model. Test retrieval before building the full RAG system. If retrieval consistently fails, the problem might be with chunking or embeddings, not the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Complete RAG Implementation with LCEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Create a RAG prompt template\n",
        "rag_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        ")\n",
        "\n",
        "# Create retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# Helper to format retrieved documents into a single string\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Build the LCEL RAG chain using the pipe (|) operator\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Ask a question\n",
        "question = \"What is machine learning?\"\n",
        "response = rag_chain.invoke(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"\\nAnswer: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: LCEL makes RAG chains composable and transparent — each step (retrieval → formatting → prompting → LLM → parsing) is explicit. This is simpler to debug and extend than legacy chain types. However, RAG still adds complexity over simple API calls. Use it when you need to query large document collections or provide domain-specific knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Comparison: With vs Without RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Without RAG: Direct API call\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "simple_response = llm.invoke(\"What is machine learning?\")\n",
        "print(\"Without RAG (direct API call):\")\n",
        "print(simple_response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# With RAG: Context from vector database (LCEL chain)\n",
        "rag_response = rag_chain.invoke(\"What is machine learning?\")\n",
        "print(\"With RAG (retrieved context):\")\n",
        "print(rag_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Compare the complexity and cost of both approaches. RAG is powerful but requires infrastructure, embeddings, and retrieval logic. Simple API calls work for many use cases. Recognize when the added complexity of RAG is justified by the requirements."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "consulting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
